---
title: "Building Patient-Level Predictive Model with MIMIC-III"
author: "Eduardo Angulo, Giovanny Barbosa, Carlos Conrado, Cristhyan De Marchena, Emmanuel Gutiérrez, Kamila Hernandez, Deivis Martínez, Omar Mejia"
date: '`r Sys.Date()`' #"26/5/2022"
output: html_document
---

# Introduction
Loren ipsum

# Running configuration
In this section we present a form of executing this vignette, by configuring a Docker container in your local machine or in a Virtual Machine (VM) in the cloud. We assume that you have access, locally or in the cloud, to the [MIMIC-III Clinical Database](https://physionet.org/content/mimiciii/1.4/) in the [OMOP Common Data Model (CDM)](https://ohdsi.github.io/TheBookOfOhdsi/CommonDataModel.html) format, thus we proceed as follows:
- If you are using a VM in the cloud you need to connect to it via SSH and establish a SSH tunnel using `ssh -L <local_port>:localhost:8787 <vm_user>@<vm_ip>`.
- Install [Docker Engine](https://docs.docker.com/engine/install/) if not installed.
- Clone the [OHDSI/PatientLevelPrediction](https://github.com/OHDSI/PatientLevelPrediction) repository.
- Make sure that the port 8787 is available on the machine that will run the Docker container.
- Run the following command `docker run -d --name=<docker_name> --network=host -v <path_to_repository>:/home/ohdsi/workdir -e USER=ohdsi -e PASSWORD=ohdsi odysseusinc/rstudio-ohdsi:latest`. If the docker user is not on sudoers list/group, you may need to run it as sudo.
- On your browser, access to `http://localhost:8787` and login to rstudio using the following credentials:
  - **Username:** ohdsi
  - **Password:** ohdsi
- We are using `PatientLevelPrediction` version 5.0.5, you can check the version of `PatientLevelPrediction` package by running `packageVersion('PatientLevelPrediction')` on the rstudio console. If you have a version mismatch run the following commands:
  - `install.packages('remotes')`.
  - `remotes::install_github('OHDSI/FeatureExtraction')`.
  - `remotes::install_github('OHDSI/PatientLevelPrediction')`.
  - If a confirmation is needed, press enter.
  - Restart the rstudio session.
- On the file explorer, search for `vignettes/BuildingPredictiveModelsMimic.Rmd` file and run it.

# Study specification

To perform this study is necessary to define the prediction problem that will be addressed, the target and outcome cohorts,  which algorithm will be implemented for the model, and how the model will be evaluated and validated. Next, a case of a patient-level predictive model is presented.

## Problem Definition

Cardiovascular Disease (CVD) is a term used to group several conditions that affect the heart or blood vessels. These are the leading cause of death globally. According to World Health Organization, about 17.9 million people died due to CVDs in 2019, which represents 32% of all global deaths. On the other hand, heart disease, a type of CVD, is the cause of about 659,000 deads in the United States, that is, one in every four. By the above, the need to develop predictive models that allow detecting the risk of the patient is observed.

In this work, the PatientLevelPrediction package is applied to observational healthcare data to address the following patient-level prediction question:

Which heart disease patients admitted into the intensive care unit (ICU) have a high risk of death?

The target cohort was defined as the patients who have been diagnosed with heart disease anytime and were admitted into ICU before the index time. On the other hand, the outcome cohort was defined as the patients that passed away.

## Study population definition

To define the population of this study is necessary to keep in mind the following:
- The observation time used in this study was defined as any time before the index time.
- In this study was defined that patients do not enter multiple times in the target cohort even if they have several diagnoses that qualified to get into it.
- Keeping in mind that the outcome of this study is death, it is not allowed the entry of patients that had experienced the outcome without getting into the target cohort.
- In this study, the predictions will be performed in a ‘time-at-risk’ window starting one day after the start of the target cohort up to 90 days later.
- A minimum time-at-risk of 30 days was defined for this study. The above was performed to include patients that do not experience the outcome earlier than the end of the time-at-risk period and exit the database.

## Model development settings
Due to the nature of our problem we have to abide to a binary classification, thus we selected two target models:

| **Algorithm**                   | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | **Hyper-parameters**                                                                                                                                                |
|---------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Regularized Logistic Regression | Lasso logistic regression belongs to the family of generalized linear models, where a linear combination of the variables is learned and finally a logistic function maps the linear combination to a value between 0 and 1. The lasso regularization adds a cost based on model complexity to the objective function when training the model. This cost is the sum of the absolute values of the linear combination of the coefficients. The model automatically performs feature selection by minimizing this cost. | `var` (starting variance), `seed`                                                                                                                                   |
| Gradient boosting machines      | Gradient boosting machines is a boosting ensemble technique and in our framework it combines multiple decision trees. Boosting works by iteratively adding decision trees but adds more weight to the data-points that are misclassified by prior decision trees in the cost function when training the next tree.                                                                                                                                                                                                    | `ntree` (number of trees), `max depth` (max levels in tree), `min rows` (minimum data points in in node), `learning rate`, `balance` (balance class labels), `seed` |

Furthermore, we have to decide on the **covariates** that we will use to train our model. In our example, we add covariates such as:
- Age
- Age Group
- Chads2
- Chads2 Vasc
- Charlson Index
- Condition Era
  - Any Time Prior
- Condition Group Era
  - Long Term
  - Any Time Prior
- Condition Occurrence
  - Short Term
  - Medium Term
  - Long Term
  - Any Time Prior
- Dcsi
- Drug Group Era
  - Long Term
  - Any Time Prior
- Gender
- Measurement
  - Any Time Prior
- Measurement Value
  - Any Time Prior
- Procedure Occurrence
  - Any Time Prior
- Race
- Visit Concept Count
  - Long Term

## Study implementation
Now we have completely designed our study, and we have to implement it. We have to generate the target and outcome cohorts and we need to develop the R code to run against our MIMIC-III database that will execute the analysis.

### Cohort instantiation
In order to load our cohorts, we leverage some of the work by generating the SQL script with [ATLAS](#atlas-cohort-builder). The result table would have the following structure:

| **Column**             | **Description**                                                                                             |
|------------------------|-------------------------------------------------------------------------------------------------------------|
| `cohort_definition_id` | A unique identifier for distinguishing between different types of cohorts, e.g. target and outcome cohorts. |
| `subject_id`           | A unique identifier corresponding to the person_id in the CDM.                                              |
| `cohort_start_date`    | The date the subject enters the cohort.                                                                     |
| `cohort_end_date`      | The date the subject leaves the cohort.                                                                     |

### ATLAS cohort builder

Firstly, go to [ATLAS atlas.ohdsi.org](https://atlas.ohdsi.org/#/home) in production, where you are asked to be logged in, select "Sign in" at the top right or log in at the center of the page.

![Figure 1: production environment](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033085/MIMIC-III/Atlas_1_p0lo6c.png){width=100%}

Then, register with a valid Google account to be able to continue on the platform.

![Figure 2: production environment - Register](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033085/MIMIC-III/Atlas_2_gxu67h.png){width=100%}

Once inside ATLAS, it is verified that we are in the **Cohort Definitions** section in the menu on the left.

![Figure 3: production environment - Cohort Definitions](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_3_ul5lgl.png){width=100%}

In the **filter text box** at the top right, filter by the name of the cohort or a keyword that allows us to find the cohort that is required. In this study we do it with the word **heart**. As shown in the figure below.

![Figure 4: production environment - Filter](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_3_2_jpxkx9.png){width=100%}

**Prevalent heart disease** is selected from the filtered options for this investigation.

![Figure 5: production environment - Select Cohort](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_4_2_nocjr0.png){width=100%}

After selecting the filters, when the **Definition** tab  appears, we go to the **Export** tab and within it select **JSON** and we mark **Copy to clipboard** to have it in memory as shown in the following image:

![Figure 6: production environment - Copy JSON](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_6_2_ftfyaa.png){width=100%}
Once the data is copied into memory as a template, it is passed to [ATLAS DEMO atlas-demo.ohdsi.org](https://atlas-demo.ohdsi.org/#/home) where its own cohort is created based on the template data that we brought.

In this platform it is not necessary to be registered, so we directly select **Cohort Definitions**.

![Figure 7: demo environment - Cohort Definitions (New Cohort)](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_7_2_wiwgzk.png){width=100%}

In this demo section we can create our own cohort by selecting **New Cohort**

![Figure 8: demo environment - New Cohort](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_8_2_h4ttys.png){width=100%}

At this point we go to **Export** within this **JSON** tab and paste what was copied into the production environment to serve as a template and mark **Reload**.

![Figure 9: demo environment - ](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_9_2_dzhsvq.png){width=100%}

With the template already copied, we proceed to name it as considered according to the study that is being carried out and we add the initial events and the attributes of the cohort that is being configured for its subsequent application.

![Figure 10: demo environment](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_10_2_bxhob0.png){width=100%}

After configuring the previous characteristics and properly naming the new cohort, it is saved in the disk button on the right side of the text box where the name was written.

Once the above is done, we go to the **Export** tab, then **SQL**, then **Template OHDSI.SQL** and copy the SQL content to take it to our **RStudio** and paste it into a file with .sql that will be taken into our model application.

![Figure 11: demo environment](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033085/MIMIC-III/Atlas_demo_11_2_psxxaz.png){width=100%}

In the present study, the [Target Cohort](https://atlas-demo.ohdsi.org/#/cohortdefinition/1779448) were defined as described in the previous images and the [Outcome Cohort](https://atlas- demo.ohdsi.org/#/cohortdefinition/1770232) found in the DEMO environment at these respective links.

It should be noted that for the **Outcome Cohort**, the procedure is the same as indicated in the previous steps as in the **Target Cohort**.

### Database connection
After we have got the SQL script from ATLAS, it's worth noticing that prior to stablishing a connection with our database we need have a JDBC driver, we can download the driver for postgres by running the following R code:
```{r echo=F results='hide'}
base_path = '/home/ohdsi'
driver_path = paste0(base_path, '/drivers')

DatabaseConnector::downloadJdbcDrivers(
  dbms = 'postgresql',
  pathToDriver = driver_path
)
```

Due to downloading the driver, among other reasons, we move our working directory and set some environment variables:
```{r echo=F results='hide'}
Sys.setenv(DATABASE_CONNECTOR_JAR_FOLDER = driver_path)
Sys.setenv(DATABASE_SERVER = 'mimic-server.postgres.database.azure.com/mimic') #'localhost/ohdsi'
Sys.setenv(DATABASE_PWD = 'securepass321.') #'supersecret'

expected_wd = paste0(base_path, '/workdir/vignettes')
if (getwd() != expected_wd){
  setwd(cat(getwd(), '/workdir/vignettes/'))  
}
```

In case of requiring more memory use, use the following command to increase the memory to be used by the JDBC drivers:
```{r}
options(java.parameters = "-Xmx2000m")
```

Then, we can establish a connection with our MIMIC-III database as follows:
```{r echo=T}
cdm_database_schema = 'omop'
target_database_schema = 'omop'
target_cohort_table = 'atlas_cohorts'
database_name = 'mimic'

connectionDetails = DatabaseConnector::createConnectionDetails(
  dbms = 'postgresql',  
  user = 'postgres',
  password = Sys.getenv('DATABASE_PWD'),
  server = Sys.getenv('DATABASE_SERVER'),
  port = 5432,
  extraSettings = 'ssl=true;',
  pathToDriver = Sys.getenv('DATABASE_CONNECTOR_JAR_FOLDER')
)

connection = DatabaseConnector::connect(connectionDetails)
DatabaseConnector::dbListTables(connection, schema = cdm_database_schema)
```

_Note: We hid our host and password fields with default values in order to block access to our MIMIC-III Database._

### Loading cohorts
Firstly, we need to create the cohort table:
```{r echo=T}
dir.create(file.path(getwd(), 'atlas_cohorts'), showWarnings = FALSE)
download.file(
  'https://gist.githubusercontent.com/demarchenac/cddd3cab8f1938173042106a6323ebd1/raw/96e774c2fa38ef7bdcada137cf57c636ed9670a2/reset_cohorts.sql',
  file.path(getwd(), 'atlas_cohorts', 'reset_cohorts.sql')
)

sql = SqlRender::readSql(file.path(getwd(), 'atlas_cohorts', 'reset_cohorts.sql'))

sql = SqlRender::render(
  sql, 
  cdm_database_schema = cdm_database_schema, 
  cohort_table = target_cohort_table
)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::executeSql(connection, sql)
```

Furthermore, we proceed as follows in order to load the target cohort:
```{r echo=T}
download.file(
  'https://gist.githubusercontent.com/demarchenac/a13e3b4832febcfe2203159560d90106/raw/03ec3f7854d495eb1e184487db13622bfd5cb6ef/target_cohort.sql', 
  file.path(getwd(), 'atlas_cohorts', 'target_cohort.sql')
)

sql = SqlRender::readSql(file.path(getwd(), 'atlas_cohorts', 'target_cohort.sql'))

sql = SqlRender::render(
  sql, 
  cdm_database_schema = cdm_database_schema, 
  target_cohort_table = target_cohort_table,
  target_database_schema = target_database_schema,
  vocabulary_database_schema = cdm_database_schema,
  target_cohort_id = '4'
)

# Manually replace codesets
gs = gsub("#Codesets", paste0(cdm_database_schema, ".codesets"), sql)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::executeSql(connection, sql)
```

Then, we repeat this process with the outcome cohort:
```{r echo=T}
download.file(
  'https://gist.githubusercontent.com/demarchenac/a6908b7a6b390a0dbc4d41265c9b03a7/raw/5e6dfbea6d95faf744e6ad8d122c4807cf668562/outcome_cohort.sql', 
  file.path(getwd(), 'atlas_cohorts', 'outcome_cohort.sql')
)

sql = SqlRender::readSql(file.path(getwd(), 'atlas_cohorts', 'outcome_cohort.sql'))

sql = SqlRender::render(
  sql, 
  cdm_database_schema = cdm_database_schema, 
  target_cohort_table = target_cohort_table,
  target_database_schema = target_database_schema,
  target_cohort_id = '3'
)

# Manually replace codesets
gs = gsub("#Codesets", paste0(cdm_database_schema, ".codesets"), sql)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::executeSql(connection, sql)
```

In addition, we can check our cohort generation with the following R code:
```{r echo=T}
sql = paste(
  'SELECT cohort_definition_id, COUNT(*) AS count',
  'FROM @output_schema.@cohort_table',
  'GROUP BY cohort_definition_id'
)

sql = SqlRender::render(
  sql, 
  output_schema = target_database_schema, 
  cohort_table = target_cohort_table
)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::querySql(connection, sql)
```

And we can view some entries with the following R code:
```{r echo=T}
sql = paste('SELECT * FROM @output_schema.@cohort_table LIMIT 5')

sql = SqlRender::render(
  sql, 
  output_schema = target_database_schema, 
  cohort_table = target_cohort_table
)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::querySql(connection, sql)

DatabaseConnector::disconnect(connection)
```

### Study script creation
In this section, we will explain how to continue the R script that will execute our study as we have defined previously. Remember that our cohorts have been created by using [ATLAS](#atlas-cohort-builder).

### Data extraction
Other inclusion criteria can also be defined to extract the relevant characteristics, for which we use the createCovariateSettings method, which we can find more information in the following [link](https://ohdsi.github.io/FeatureExtraction/reference/createCovariateSettings.html) 

```{r}
covariateSettings <- FeatureExtraction::createCovariateSettings(
  useDemographicsGender = TRUE,
  useDemographicsAge = TRUE,
  useConditionGroupEraLongTerm = TRUE,
  useConditionGroupEraAnyTimePrior = TRUE,
	useDrugGroupEraLongTerm = TRUE,
  useDemographicsRace = TRUE,
	useConditionOccurrenceAnyTimePrior = TRUE,
	useConditionOccurrenceLongTerm = TRUE,
	useConditionOccurrenceMediumTerm = TRUE,
	useConditionOccurrenceShortTerm = TRUE,
	useDemographicsAgeGroup = TRUE,
	useConditionEraAnyTimePrior = TRUE,
	useProcedureOccurrenceAnyTimePrior = TRUE,
	useMeasurementAnyTimePrior = TRUE,
	useMeasurementValueAnyTimePrior = TRUE,
	useDrugGroupEraAnyTimePrior = TRUE,
	useCharlsonIndex = TRUE,
	useDcsi = TRUE,
	useChads2 = TRUE,
	useChads2Vasc = TRUE,
	useVisitConceptCountLongTerm = TRUE,
	longTermStartDays = -365,
	endDays = -1
)
```

Create a configuration that contains the details about the cdmDatabase connection for data extraction, for [more information](https://ohdsi.github.io/PatientLevelPrediction/reference/createDatabaseDetails.html) 
```{r}
databaseDetails <- PatientLevelPrediction::createDatabaseDetails(
  connectionDetails = connectionDetails,
	cdmDatabaseSchema = cdm_database_schema,
	cdmDatabaseName = database_name,
	cohortDatabaseSchema = target_database_schema,
  cohortTable = target_cohort_table,
	cohortId = 4,
	outcomeDatabaseSchema = target_database_schema,
	outcomeTable = target_cohort_table,
	outcomeIds = 3,
	cdmVersion = 5
)
```

We restrict the sample size if we do not feel we have enough computing power or if we only want to run a small sample:
	
```{r}
restrictPlpDataSettings <- PatientLevelPrediction::createRestrictPlpDataSettings(sampleSize = 100000)
```

This function executes a large set of SQL statements against the database in OMOP CDM format to extract the data needed to perform the analysis [more info](https://rdrr.io/github/OHDSI/PatientLevelPrediction/man/getPlpData.html).
Once the necessary data has been extracted, we save it in case we want to replicate the example:

```{r}
dir.create(file.path(getwd(), 'atlas_plp_data'), showWarnings = FALSE)

plpData <- PatientLevelPrediction::getPlpData(
  databaseDetails = databaseDetails,
	covariateSettings = covariateSettings,
	restrictPlpDataSettings = restrictPlpDataSettings
)
	
PatientLevelPrediction::savePlpData(plpData, file.path(getwd(), 'atlas_plp_data'))
```

### Additional inclusion criteria
The configuration for the study population is created, which details information such as the start and end of the cut, risk window and the outcome. For more information about the method you can read its [documentation](https://www.rdocumentation.org/packages/PatientLevelPrediction/versions/4.3.10/topics/createStudyPopulationSettings). 
```{r}
populationSettings <- PatientLevelPrediction::createStudyPopulationSettings(
  washoutPeriod = 0,
	firstExposureOnly = TRUE,
	removeSubjectsWithPriorOutcome = TRUE,
	priorOutcomeLookback = 99999,
	riskWindowStart = 1,
	riskWindowEnd = 90,
	startAnchor = 'cohort start',
  endAnchor = 'cohort start',
	minTimeAtRisk = 30,
	requireTimeAtRisk = TRUE,
	includeAllOutcomes = TRUE
)
```

### Spliting the data into training/validation/testing datasets
Continuing with the steps for model training, we divide the data into training, validation and test sets, this with the values defined for cross-validation.
```{r}
splitSettings <- PatientLevelPrediction::createDefaultSplitSetting(
  trainFraction = 0.75,
	testFraction = 0.25,
	type = 'stratified',
	nfold = 5, 
	splitSeed = 1234
)
```

### Preprocessing the training data
 
It was used the default sample settings. it simply returns the trainData as input, see below: 

```{r tidy=FALSE, eval=FALSE}
sampleSettings <- PatientLevelPrediction::createSampleSettings()
```

However, the current package contains methods of under-sampling the non-outcome patients.  To perform undersampling, the `type` input should be 'underSample' and `numberOutcomestoNonOutcomes` must be specified (an integer specifying the number of non-outcomes per outcome).  It is possible to add any custom function for over/under sampling, see [vignette for custom sampling](https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/AddingCustomSamples.pdf).

It is possible to specify a combination of feature engineering functions that take as input the trainData and output a new trainData with different features.  The default feature engineering setting does nothing:

```{r tidy=FALSE, eval=FALSE}
featureEngineeringSettings <- PatientLevelPrediction::createFeatureEngineeringSettings()
```

Finally, the preprocessing settings.  For this setting it was defined `minFraction = 0.01`, this removes any features that is observed in the training data for less than 0.01 fraction of the patients. The input `normalize = T` specifies whether the features are scaled between 0 and 1.  The input `removeRedundancy = T` specifies whether features that are observed in all of the target population are removed.

```{r tidy=FALSE, eval=FALSE}
preprocessSettings <- PatientLevelPrediction::createPreprocessSettings(
	minFraction = 0.01, 
	normalize = T, 
	removeRedundancy = T
)
```

### Model development

For the algorithm selection with considered two options:

- [setLassoLogisticRegression](https://ohdsi.github.io/PatientLevelPrediction/reference/setLassoLogisticRegression.html) : [LASSO regression](https://en.wikipedia.org/wiki/Lasso_(statistics)) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.

- [setGradientBoostingMachine](https://ohdsi.github.io/PatientLevelPrediction/reference/setGradientBoostingMachine.html) : [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees.

For our problem we choose to build a logistic regression model with the default hyper-parameters
```{r tidy=TRUE, eval=FALSE}
lrModel <- PatientLevelPrediction::setLassoLogisticRegression()
```

For the gradient boosting machines model it was used the hyper-parameters explained in [vignette for building patient-level predictive models](https://github.com/OHDSI/PatientLevelPrediction/blob/main/inst/doc/BuildingPredictiveModels.pdf).

```{r tidy=TRUE, eval=FALSE}
lrModel <- PatientLevelPrediction::setGradientBoostingMachine(
  ntrees = 5000, 
  maxDepth = c(4, 7, 10), 
  learnRate = c(0.001, 0.01, 0.1, 0.9)
)
```

The `runPlP` function requires the `plpData`, the `outcomeId` specifying the outcome being predicted and the settings: `populationSettings`, `splitSettings`, `sampleSettings`, `featureEngineeringSettings`, `preprocessSettings` and `modelSettings` to train and evaluate the model. 

```{r tidy=FALSE, eval=FALSE}
dir.create(file.path(getwd(), 'atlas_plp_model'), showWarnings = FALSE)

lrResults <- PatientLevelPrediction::runPlp(
	plpData = plpData,
	outcomeId = 3, 
	analysisId = 'Test',
	analysisName = 'Demonstration of runPlp for training single PLP models',
	populationSettings = populationSettings, 
	splitSettings = splitSettings,
	sampleSettings = sampleSettings, 
	featureEngineeringSettings = featureEngineeringSettings, 
	preprocessSettings = preprocessSettings,
	modelSettings = lrModel,
	logSettings = PatientLevelPrediction::createLogSettings(), 
	executeSettings = PatientLevelPrediction::createExecuteSettings(
		runSplitData = T, 
		runSampleData = T, 
		runfeatureEngineering = T, 
		runPreprocessData = T, 
		runModelDevelopment = T, 
		runCovariateSummary = T
	), 
	saveDirectory = file.path(getwd(), 'atlas_plp_model')
)
```


### Result analysis
```{r}
library(PatientLevelPrediction)
library(dplyr)
viewPlp(lrResults)
```
